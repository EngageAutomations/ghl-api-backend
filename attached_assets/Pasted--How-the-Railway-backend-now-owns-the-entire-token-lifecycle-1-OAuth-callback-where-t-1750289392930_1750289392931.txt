### How the Railway backend now **owns the entire token lifecycle**

1. **OAuth callback – where tokens first arrive**

   * `/oauth/callback` and `/api/oauth/callback` exchange the `code` for
     `{access_token, refresh_token, expires_in, locationId, scope}`.
   * A new **installation record** is inserted into the in-memory `Map`:

     ```
     {
       id, accessToken, refreshToken,
       expiresIn,                  // seconds
       expiresAt: Date.now()+expiresIn*1000,   // ms epoch
       locationId, scopes, tokenStatus:'valid'
     }
     ```
   * `scheduleTokenRefresh(id)` is called immediately.

2. **`scheduleTokenRefresh()` – the self-resetting timer**

   * Calculates **“expiresAt − 5 minutes”** (the padding is configurable).
   * Sets a `setTimeout` that will fire at that moment and invoke `refreshAccessToken(id)`.
   * Clears and replaces any older timer if a new token comes in.

3. **`refreshAccessToken()` – the automatic renewal**

   * Sends a `grant_type=refresh_token` POST to
     `https://services.leadconnectorhq.com/oauth/token`.
   * On success it **overwrites** `accessToken`, `refreshToken`, `expiresIn`, `expiresAt`, and flags `tokenStatus:'valid'`.
   * Immediately calls `scheduleTokenRefresh(id)` to arm the next cycle.
   * On failure it logs `[REFRESH-FAIL]` and flips `tokenStatus:'invalid'` (routes will now reject calls for that install until the user re-authenticates).

4. **`ensureFreshToken(id)` – on-demand safety net**

   * Runs at the top of every GHL-proxy route.
   * If token expires in < 5 min **or** no `expiresAt` is present, it performs an *inline* refresh so the request never goes out with a stale token.
   * Throws if refresh fails, which the route converts to a 400/500 JSON error.

5. **Making the outbound call**

   * After `ensureFreshToken`, the route uses

     ```
     Authorization: Bearer <installation.accessToken>
     ```

     with full confidence that the token is live for \~24 h.

6. **Container (re)start**

   * When `app.listen()` fires, the code iterates over every record in the `installations` `Map` and re-runs `scheduleTokenRefresh(id)`.
   * **If you persisted the installs** (DB/Redis/File), reload them first; timers come back automatically.
   * **If you kept RAM-only storage**, installs are lost on each deploy → users must re-OAuth.

7. **Concurrency & scaling**

   * Single Railway instance → nothing more to do.
   * Multiple replicas → each would try to refresh; use a distributed lock (e.g. Redlock on Redis) to let only one succeed.

8. **Observability**

   * Success: `[REFRESH] install_123 – new token good for 23.9 h`
   * Failure: `[REFRESH-FAIL] install_123 {error json}`
   * Alert on refresh-fail spikes to spot revoked tokens or client-secret issues.

9. **Security notes**

   * Tokens never leave the backend; the frontend holds only an `installation_id`.
   * All secrets (`GHL_CLIENT_*`) are Railway env-vars, never logged.
   * Log statements redact the actual token string.

**Bottom line:** once an installation finishes OAuth, the backend takes full responsibility for keeping its access token valid—via a combo of scheduled refreshes, on-demand refresh guards, and timer re-arming on boot—so every downstream API call made through Railway is always authenticated without any manual token juggling.
